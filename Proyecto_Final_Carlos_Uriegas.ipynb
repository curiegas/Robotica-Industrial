{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto Final Robotica Industrial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQg72iJuEaZN",
        "colab_type": "text"
      },
      "source": [
        "**Proyecto Final**\n",
        "\n",
        "Robotica Industrial\n",
        "\n",
        "Carlos Andres Uriegas Garcia de Alba\n",
        "\n",
        "Michel Emanuel López Franco\n",
        "\n",
        "25 de abril de 2020\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMqtXo0PFSIf",
        "colab_type": "text"
      },
      "source": [
        "En este proyecto se presenta una manera de trabajar deteccion y reconocimiento facial a traves de la camara de la computadora en Google Colab. Como el objetivo del proyecto es tanto la transmisión de video como la detección de rostros dentro de un navegador web, se utilizo una combinacion de Python y Javascript. El programa toma un video del cual captura imagenes y las procesa para entrenar con ellas y asociar un nombre a la persona en las imagenes. Despues el programa toma un video de prueba en el que se busca identificar si la persona en el video correpsonde a alguna persona previamente registrada a traves del entrenamiento. Si es asi, el programa presenta una imagen del video de prueba en la que un recuadro enmarca la cara de la persona y un texto en el recuadro indica el nombre de la persona enmarcada. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMjMtmQC0iRr",
        "colab_type": "text"
      },
      "source": [
        "Antes de comenzar, se debe de descargar un zip con el nombre \"face-detection-master.zip\" que se encuentra en la siguiente liga https://drive.google.com/open?id=1Ox3B9MGAWOOnMLVyzXGBGUmMZFL2i_R3 .\n",
        "Despues descomprima el archivo \"face-detection-master.zip\", copie la carpeta \"Face-Detection-Master\" en su Google Drive y cambie el nombre de la carpeta a \"Colab Notebooks\". \n",
        "Si es necesario, cambie el nombre de la subcarpeta llamada \"Carlos\" (mi nombre) y cambiela en el codigo. Esta es la carpeta que almacena las imágenes .png (primeros 30 cuadros) extraidas del video de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAlOqkxpGk6m",
        "colab_type": "text"
      },
      "source": [
        "Las primeras dos lineas de codigo envian al usuario a confirmar que el programa puede escribir y leer archivos de su cuenta de Google Drive. Ahi se guardaran los videos capturados y los archivos necesarios para el programa. Esto es necesario para mantener el programa totalmente en linea. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZML9NvImqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDThxYTHIpR5",
        "colab_type": "text"
      },
      "source": [
        "Las siguientes lineas de codigo son una combinacion de Python y Javascript que inicializa variables globales, administra la camara de la computadora, prepara las capturas de video, conecta la transmision de la camara a un html, y la muestra en la pantalla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElD2bB0tJ54N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ffmpeg-python\n",
        "\n",
        "\n",
        "from IPython.display import HTML, Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "video_file_train = '/content/drive/My Drive/Colab Notebooks/Video Datasets/osy_train.mp4' \n",
        "video_file_test = '/content/drive/My Drive/Colab Notebooks/Video Datasets/osy_test.mp4' \n",
        "  \n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var my_btn_txt = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(my_btn_txt);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, videoStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  videoStream = stream;\n",
        "  var options = {  \n",
        "    mimeType : 'video/webm;codecs=vp9'  \n",
        "  };            \n",
        "  recorder = new MediaRecorder(stream, options);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('video');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({video: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      videoStream.getVideoTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... Please wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def start_webcam():\n",
        "  js = Javascript('''\n",
        "    async function startWebcam() {\n",
        "      const div = document.createElement('div');\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      \n",
        "      return;\n",
        "\n",
        "    }\n",
        "    ''')\n",
        "  \n",
        "  display(js)\n",
        "  data = eval_js('startWebcam()')\n",
        "  \n",
        "    \n",
        "start_webcam()\n",
        "\n",
        "def get_video():\n",
        "  display(HTML(VIDEO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  return binary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKORWmoAKMzl",
        "colab_type": "text"
      },
      "source": [
        "Las siguientes lineas de codigo inician la grabacion de la transmisión de video de la camara y la convierten en datos binarios a traves de la funcion *get_video()* definida previemnte. Una vez que se presiona el boton para detener la grabacion, el programa retorna los datos binarios y se guardan como un archivo .mp4. que es almacenado en la carpeta llamada \"Video Dataset\" del Google Drive asociado previamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "446V0_wfMwpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "videofile = get_video()\n",
        "\n",
        "with open(video_file_train, 'wb') as f:\n",
        "  f.write(videofile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdg1SmVvMzKE",
        "colab_type": "text"
      },
      "source": [
        "Las siguientes lineas de codigo son las mismas que las anteriores, la diferencia es que ahora graban el video que sera usado para la deteccion facial. Es decir, la cara que se identifique en este video sera comparada con la cara previamente almacenada en las lineas anteriores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmAXD36Odhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "videofile = get_video()\n",
        "\n",
        "with open(video_file_test, 'wb') as f:\n",
        "  f.write(videofile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psn7CdJ3Of1u",
        "colab_type": "text"
      },
      "source": [
        "Las siguientes lineas de codigo hacen uso de los videos de entrenamiento y de prueba. \n",
        "\n",
        "Primero se extraen los primeros 30 cuadros del video de entrenamiento y se almacenan como imagenes .png en la carpeta llamada \"Image Dataset\" del Google Drive asociado previamente. Cada imagen se convertira de color a una escala de grises. Luego, las caras seran pre-detectadas por un algoritmo OpenCV, *CascadeClassifier.detectMultiScale()*. Este algoritmo permite detectar objetos y devuelve una lista de rectángulos. OpenCV usa un clasificador para detectar objetos particulares llamado *haarcascade_frontalface_default.xml*. El *haarcascade_frontalface_default.xml* es un archivo xml haar cascade diseñado por OpenCV para detectar la cara frontal. Finalmente, las imagenes se redimensionan a un formato mas pequeño y se almacenan en la carpeta \"Image Dataset / Carlos\" de Google Drive (puede cambiar en el codigo el nombre \"Carlos\" a el nombre con el que desee asociar las imagenes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxvwW642wroG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2, sys, numpy, os\n",
        "from time import sleep\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# OpenCV Default config file for face detection\n",
        "haar_file = '/content/drive/My Drive/Colab Notebooks/Config/haarcascade_frontalface_default.xml'\n",
        "  \n",
        "# Root Folder used to store the face data images\n",
        "datasets = '/content/drive/My Drive/Colab Notebooks/Image Datasets'\n",
        " \n",
        "# Subfolder used to store my images (to be changed with your name)\n",
        "sub_data = 'Carlos'     \n",
        "  \n",
        "path = os.path.join(datasets, sub_data) \n",
        "if not os.path.isdir(path): \n",
        "    os.mkdir(path) \n",
        "  \n",
        "# define the size of images  \n",
        "(width, height) = (130, 100)     \n",
        "  \n",
        "# Analyse the video training file \n",
        "face_cascade = cv2.CascadeClassifier(haar_file) \n",
        "webcam = cv2.VideoCapture(video_file_train)  \n",
        "  \n",
        "# Capture the first 30 images of the train video\n",
        "count = 1\n",
        "while count < 30: \n",
        "    if not webcam.isOpened():\n",
        "        print('Unable to load the video file.')\n",
        "        sleep(5)\n",
        "        pass  \n",
        "    (_, im) = webcam.read() \n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) \n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 4) \n",
        "    for (x, y, w, h) in faces: \n",
        "        cv2.rectangle(im, (x, y), (x + w, y + h), (255, 0, 0), 2) \n",
        "        face = gray[y:y + h, x:x + w] \n",
        "        face_resize = cv2.resize(face, (width, height)) \n",
        "        cv2.imwrite('% s/% s.png' % (path, count), face_resize) \n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdyS3sRUw24l",
        "colab_type": "text"
      },
      "source": [
        "Las siguientes lineas de codigo entrenan con las imagenes extraidas del video de entrenamiento e intentan reconocer la cara de la persona en el video de prueba. \n",
        "\n",
        "Primero, se analiza la carpeta \"Image Dataset\" y sus subcarpetas para crear una lista de imagenes almacenadas y su nombre correspondiente. Luego, OpenCV entrena con esta lista de imagenes y sus nombres correspondientes. Finalmente, *CascadeClassifier* se usa para detectar la cara. \n",
        "\n",
        "Posteriormente, los primeros 30 cuadros del video de prueba se utilizan para la deteccion de rostros. Cada uno de ellos es analizado con los metodos *detectMultiScale()* y *predict()* para reconocer la cara. El nombre de la persona detectada se muestra con un rectangulo y un indicador de confianza de prediccion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoMXLG-T0Gmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# User OpenCV Face Recognizer to train the AI & Cascade Classifier to detect faces\n",
        "  \n",
        "# Create a list of images and a list of corresponding names \n",
        "(images, lables, names, id) = ([], [], {}, 0) \n",
        "for (subdirs, dirs, files) in os.walk(datasets): \n",
        "    for subdir in dirs: \n",
        "        names[id] = subdir \n",
        "        subjectpath = os.path.join(datasets, subdir) \n",
        "        for filename in os.listdir(subjectpath): \n",
        "            path = subjectpath + '/' + filename \n",
        "            lable = id\n",
        "            images.append(cv2.imread(path, 0)) \n",
        "            lables.append(int(lable)) \n",
        "        id += 1\n",
        "(width, height) = (130, 100) \n",
        "  \n",
        "# Create a Numpy array from the two lists above \n",
        "(images, lables) = [numpy.array(lis) for lis in [images, lables]] \n",
        "  \n",
        "# Train the model from the images with OpenCV Face Recognizer\n",
        "model = cv2.face.LBPHFaceRecognizer_create()\n",
        "model.train(images, lables) \n",
        "\n",
        "#print(lables, names, id)\n",
        "\n",
        "# Use OpenCV Cascade Classifier to detect faces \n",
        "face_cascade = cv2.CascadeClassifier(haar_file) \n",
        "webcam = cv2.VideoCapture(video_file_test)\n",
        "\n",
        "count = 1\n",
        "while count < 30:\n",
        "    (_, im) = webcam.read() \n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) \n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
        "    for (x, y, w, h) in faces: \n",
        "        cv2.rectangle(im, (x, y), (x + w, y + h), (255, 0, 0), 2) \n",
        "        face = gray[y:y + h, x:x + w] \n",
        "        face_resize = cv2.resize(face, (width, height)) \n",
        "        # Try to recognize the face \n",
        "        prediction = model.predict(face_resize) \n",
        "        print(prediction)\n",
        "        cv2.rectangle(im, (x, y), (x + w, y + h), (0, 255, 0), 3) \n",
        "  \n",
        "        if prediction[1]<100: \n",
        "           cv2.putText(im, '% s - %.0f' % (names[prediction[0]], prediction[1]), (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0)) \n",
        "           cv2_imshow(im)\n",
        "           break\n",
        "        else: \n",
        "          cv2.putText(im, 'not recognized', (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0)) \n",
        "  \n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}